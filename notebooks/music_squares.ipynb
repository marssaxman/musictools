{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has to go in its own cell or it screws up the defaults we'll set later\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music visualization for image classification\n",
    "\n",
    "Music classification and similarity detection seems like an obvious application of machine learning, but it's a hard problem and there is still [a lot of research work going on](https://magenta.tensorflow.org/). With image analysis, by contrast, there are lots of well-understood networks, some of them [included with Keras](https://keras.io/applications/), which means that you can set up an image classifier in about a dozen lines of Python. Here's a simple demo I put together in 2017, in fact, that lets you [try them all out using your webcam](https://github.com/plaidml/plaidvision). \n",
    "\n",
    "Could take advantage of this by generating pictures of audio waveforms and training an image classificaton network to identify them? Let's try out some different techniques for rendering music in a format suitable for an image classifier and see if we can find a representation that makes sense to humans. If we can see the patterns in the images we've generated, and we can readily distinguish the differences between images generated for different pieces of music, it seems probable that a robot might be able to learn to do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up\n",
    "\n",
    "We'll be using Python for this project, with [NumPy](https://pypi.org/project/numpy/), [matplotlib](https://pypi.org/project/matplotlib/), and [LibROSA](https://pypi.org/project/librosa/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa, librosa.display\n",
    "import matplotlib\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize']=(15.0, 3.0)\n",
    "matplotlib.rcParams['image.cmap']='magma'\n",
    "pyplot.rcParams['image.cmap']='magma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine a variety of other music later, but for now let's just use the example file that comes with librosa. An audio file is basically just one long array of amplitude samples and a \"sampling rate\" parameter specifying the number of samples per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = \"/home/mars/Music/Track Library/04 Groove And Move.mp3\"\n",
    "file_path = \"/home/mars/Music/Track Library/Tribone & Whitebear - Pure Self.mp3\"\n",
    "signal, sr = librosa.load(file_path)\n",
    "print \"The audio data is a %s and its length is %d.\" % (type(signal), len(signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting audio features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the most straightforward approach by simply plotting the amplitudes, producing a classic waveform image like many you've seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveplot(signal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This view tells us something about the structure of the music, but it's not very detailed. All we see are the limits of the waveforms. What's going on inside? Let's get a more detailed look by constructing an amplitude histogram. We'll slice the signal into frames, divide the amplitude range into bins, and count the number of samples in each frame which land in that bin. We can plot the resulting 2D array by applying a color map, so the brightness or darkness of each point is proportional to the number of values in the bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.array_split(signal, 1000)\n",
    "amplitude_histogram = np.array([bins for bins, edges in (np.histogram(f, bins=100) for f in frames)])\n",
    "# Convert to float and transpose from [frames, bins] to [bins, frames] order.\n",
    "amplitude_histogram = amplitude_histogram.astype(np.float).transpose()\n",
    "# Normalize by dividing each value against the maximum in its column.\n",
    "amplitude_histogram /= amplitude_histogram.max(axis=0)\n",
    "librosa.display.specshow(amplitude_histogram, cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This certainly brings out more of the structure; we can clearly see the difference between a generally quiet section which contains some loud peaks and a section which is loud overall, and we can make some guesses about rhythm patterns. We still don't know very much about the character of the sound, though: we know nothing about its pitches or timbres.\n",
    "\n",
    " We can extract the frequency content from the signal using an algorithm called the \"short-time Fourier transform\", or STFT. This process splits the array of samples into evenly-sized frames, computes the Fourier transform on each frame, and returns a two-dimensional array of frequency components by frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "spectrogram = librosa.stft(signal, n_fft=n_fft)\n",
    "print \"The spectrogram contains %d frequency components for each of %d frames.\" % spectrogram.shape\n",
    "print \"The array value type is '%s'.\" % spectrogram.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fourier transform returns complex numbers including the magnitude and phase for each frequency component. This is more information than we need, so we'll extract the magnitude and throw away the phase by taking the absolute value, allowing us to simplify things by using ordinary floats instead of complex numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = np.abs(spectrogram).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a look at this spectrogram. Since it's a two-dimensional array, we'll draw frames horizontally and frequency components vertically, representing magnitude with a color map from dark to bright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(spectrogram, cmap='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we see is a few dots along the bottom - that can't be right! What happened?\n",
    "\n",
    "Human senses are logarithmic, not linear, so we perceive small differences between small numbers to be about the same as large differences between large numbers. The levels of different frequencies in an audio signal may be a dozen orders of magnitude apart. We can clearly distinguish subtle changes in quiet frequencies while also hearing the large changes in the dominant frequencies, but when we try to plot those amplitudes on a linear scale, the loudest bands drown everything else out.\n",
    "\n",
    "Let's try to get a better model of perception by putting these amplitudes on a log scale, converting them to decibels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "librosa.display.specshow(db_spectrogram, cmap='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting closer, but this still looks pretty strange. It's not just our sense of volume that is logarithmic, but our sense of pitch, too. We are far more sensitive to the the differences in lower frequencies than higher ones. One common logarithmic model of pitch sensitivity is called the Mel scale. Let's use a mel-scaled filterbank to condense our spectrogram down to a smaller array of more useful numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filterbank which will convert an array [n_fft,frames] to [n_mels,frames].\n",
    "# This filterbank will be an array of weights [n_mels, n_fft].\n",
    "melfilterbank = librosa.filters.mel(sr, n_fft=n_fft, n_mels=24, fmin=50.0, fmax=8000.0)\n",
    "mel_spectrogram = melfilterbank.dot(spectrogram)\n",
    "db_mel_spectrogram = librosa.amplitude_to_db(mel_spectrogram)\n",
    "librosa.display.specshow(db_mel_spectrogram, cmap='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a difference between activity in low, middle, and high frequencies, and we can even get some sense of melodic variations and other musical events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perceptual weighting on the mel-scaled power spectrogram\n",
    "mel_frequencies = librosa.mel_frequencies(n_mels=24, fmin=50.0, fmax=8000.0)\n",
    "perceptual_melspec = librosa.perceptual_weighting(mel_spectrogram**2, mel_frequencies, ref=np.max)\n",
    "librosa.display.specshow(perceptual_melspec, cmap='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating images for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with these basic techniques for turning sounds into images, let's consider the problem of preparing input images for an image classification network.\n",
    "\n",
    "Each network is designed for a specific input size, which is usually no more than 224x224 pixels - Xception goes up to 299x299 pixels. That creates some challenges for us; it's not a lot of space, and the square aspect ratio doesn't really suit the horizontal, timeline-based approaches we've been using. Let's see what happens if we simply squash our existing plots into shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(dpi=100, figsize=(9,3))\n",
    "pyplot.subplot(131).imshow(amplitude_histogram, extent=[0,224,0,224], cmap='Blues')\n",
    "pyplot.subplot(132).imshow(db_mel_spectrogram, extent=[0,224,0,224], cmap='magma', origin='lower')\n",
    "pyplot.subplot(133).imshow(perceptual_melspec, extent=[0,224,0,224], cmap='magma', origin='lower')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty thumbnail image, but the horizontal compression has sacrificed so much time resolution that there's  not enough information here to recognize a track, much less to guess what kind of music it might be, or to estimate the similarity between two different pieces of music.\n",
    "\n",
    "What's more, music is full of repeating patterns and variations on patterns, which all disappear in this view. The timeline view places these repetitions apart in time, even though they could be considered very close in terms of musical similarity. If we are interested in training a classifier network to recognize musical similarity, we should find some way to make the hierarchy of rhythmic structure more prominent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tempo synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we try to synchronize our display with tempo. If we're trying to create an image that has 224x224 pixels, we could use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo = librosa.beat.tempo(signal, sr=sr)[0]\n",
    "print tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo = 128.0\n",
    "seconds_per_beat = 60.0 / tempo\n",
    "samples_per_beat = seconds_per_beat * float(sr)\n",
    "total_beats = len(signal) / samples_per_beat\n",
    "print \"samples_per_beat == %.2f, total_beats == %.2f\" % (samples_per_beat, total_beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerize_spectrogram(spec):\n",
    "    out = np.zeros_like(spec)\n",
    "    midpoint = int(spec.shape[0] / 2)\n",
    "    out[midpoint:,...] = spec[0::2,...]\n",
    "    out[midpoint-1::-1,...] = spec[1::2,...]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_wrapped(number_of_rows):\n",
    "    # Compute the number of beats which is the nearest power of 2.\n",
    "    beats_per_row = 2 ** int(np.floor(  np.log(total_beats / number_of_rows) / np.log(2.0)  ))\n",
    "    beats_to_use = beats_per_row * number_of_rows\n",
    "\n",
    "    # set the hop length so that each row will divide evenly into 224 slices\n",
    "    samples_per_row = int(beats_per_row * samples_per_beat)\n",
    "    hop_length = samples_per_row / 224\n",
    "    while hop_length >= 2048:\n",
    "        hop_length /= 2\n",
    "        \n",
    "    samples_to_use = int(np.floor(beats_to_use * samples_per_beat))\n",
    "    sample_start = int((len(signal) - samples_to_use) / 2)\n",
    "    sample_end = sample_start + samples_to_use\n",
    "    display_clip = signal[sample_start:sample_end]\n",
    "\n",
    "    pixels_per_row = 224 / number_of_rows\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=display_clip, sr=sr, n_mels=pixels_per_row, fmin=50.0, fmax=8000, hop_length=hop_length)\n",
    "    mel_frequencies = librosa.mel_frequencies(n_mels=pixels_per_row, fmin=50.0, fmax=8000.0)\n",
    "    display_spec = librosa.perceptual_weighting(mel_spec**2, mel_frequencies, ref=np.max)\n",
    "\n",
    "    frames_per_row = int(np.floor(display_spec.shape[1] / number_of_rows))\n",
    "    row_specs = [display_spec[:,i*frames_per_row:(i+1)*frames_per_row] for i in range(number_of_rows)]\n",
    "    row_specs = [centerize_spectrogram(s) for s in row_specs]\n",
    "    row_specs = np.concatenate(row_specs, axis=0)\n",
    "    \n",
    "    # if we have surplus columns, average them together\n",
    "    while row_specs.shape[1] > 224:\n",
    "        print \"condensing columns from %d to %d\" % (row_specs.shape[1], row_specs.shape[1] / 2)\n",
    "        row_specs = (row_specs[:,0:-1:2] + row_specs[:,1::2]) / 2.0\n",
    "        \n",
    "    print \"number_of_rows == %d, row_specs == %s\" % (number_of_rows, row_specs.shape)\n",
    "    return row_specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like rows and columns to be proportional.\n",
    "# The number of columns must be a power of 2.\n",
    "# The number of rows must divide evenly into 224.\n",
    "estimated_dimension = np.sqrt(total_beats)\n",
    "# Round down to the nearest integer which is a factor of 224\n",
    "# and no smaller than 7.\n",
    "pixels_per_row = max(7, int(np.ceil(224.0 / estimated_dimension)))\n",
    "number_of_rows = int(np.floor(224.0 / pixels_per_row))\n",
    "\n",
    "tiled_image = render_wrapped(number_of_rows)\n",
    "\n",
    "pyplot.figure(figsize=(6,6))\n",
    "pyplot.imshow(tiled_image, cmap='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapping linear time through planar space\n",
    "wrapping rows\n",
    "quadtree decomposition\n",
    "the hilbert curve\n",
    "self-similarity matrix\n",
    "\n",
    "tempo synchronization: is it a win?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### three dimensions of depth: color\n",
    "different color spaces: RGB, HSV, HSL, Lab\n",
    "different audio features: magnitude, power; spectral centroid, spread, rolloff, flatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiled_spectrogram(num_tiles, tile_size):\n",
    "    hop_length = int(len(signal) / (num_tiles * tile_size))\n",
    "    merge_factor = 1\n",
    "    while hop_length >= 2048:\n",
    "        hop_length /= 2\n",
    "        merge_factor *= 2\n",
    "    # compute a perceptually-scaled spectrogram with the specified hop length\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=signal, sr=sr, n_mels=tile_size, fmin=50.0, fmax=8000, hop_length=hop_length)\n",
    "    mel_frequencies = librosa.mel_frequencies(n_mels=tile_size, fmin=50.0, fmax=8000.0)\n",
    "    display_spec = librosa.perceptual_weighting(mel_spec**2, mel_frequencies, ref=np.max)\n",
    "    display_spec = centerize_spectrogram(display_spec)\n",
    "    # average out adjacent frames until it has the right length\n",
    "    while merge_factor > 1:\n",
    "        display_spec = (display_spec[:,0:-1:2] + display_spec[:,1::2]) / 2.0\n",
    "        merge_factor /= 2\n",
    "    return np.array_split(display_spec[:,:num_tiles*tile_size], num_tiles, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacefill(matrix):\n",
    "    x, y = 1, 1\n",
    "    while x < matrix.shape[1] or y < matrix.shape[0]:\n",
    "        nx = min(matrix.shape[1], x*2)\n",
    "        if nx > x:\n",
    "            clipwidth = nx - x\n",
    "            source = matrix[0:y, 0:clipwidth]\n",
    "            matrix[0:y, x:nx] = source + source.size\n",
    "            x = nx\n",
    "\n",
    "        ny = min(matrix.shape[0], y*2)\n",
    "        if ny > y:\n",
    "            clipheight = ny - y\n",
    "            source = matrix[0:clipheight, 0:x]\n",
    "            matrix[y:ny, 0:x] = source + source.size\n",
    "            y = ny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this matrix is an indexer from 0 through the number of items it contains.\n",
    "# we'll divide it by its number of items to get a 0..1 range, then scale it\n",
    "# up to the number of frames in our spectrogram, so that each cell will\n",
    "# select one frame.\n",
    "\n",
    "matrix = np.zeros(shape=(28,28), dtype=np.int)\n",
    "spacefill(matrix)\n",
    "\n",
    "number_of_cells = matrix.size\n",
    "cell_dimension = 224 / 28\n",
    "spec_tiles = tiled_spectrogram(number_of_cells, cell_dimension)\n",
    "reshaped_spec = np.asarray(spec_tiles)[matrix.reshape(-1),:,:].reshape(224,224)\n",
    "pyplot.figure(figsize=(6,6))\n",
    "pyplot.imshow(reshaped_spec, cmap='magma');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hilbert_path(power):\n",
    "    path = [(0, 0)]\n",
    "    for i in range(power):\n",
    "        s = 1 + (path[-1][0] - path[0][0]) / 2\n",
    "        path = ( [(y-s, x-s) for x,y in path]\n",
    "               + [(x-s, y+s) for x,y in path]\n",
    "               + [(x+s, y+s) for x,y in path]\n",
    "               + [(s-y,-x-s) for x,y in path] )\n",
    "    return path\n",
    "\n",
    "def hilbert_matrix(power):\n",
    "    size = 2 ** power\n",
    "    matrix = np.zeros((size, size), dtype=np.int)\n",
    "    for i, (x, y) in enumerate(hilbert_path(power)):\n",
    "        x = (x - 1 + size) / 2\n",
    "        y = (y - 1 + size) / 2\n",
    "        matrix[x,y] = i\n",
    "    return matrix\n",
    "    \n",
    "#pyplot.figure(figsize=(8,8))\n",
    "#pts = hilbert_path(4)\n",
    "#pyplot.plot(*zip(*pts))\n",
    "#pyplot.show()\n",
    "\n",
    "number_of_cells = 16 * 16\n",
    "cell_dimension = 224 / 16\n",
    "spec_tiles = tiled_spectrogram(number_of_cells, cell_dimension)\n",
    "\n",
    "matrix = hilbert_matrix(4)\n",
    "print matrix\n",
    "reshaped_spec = np.asarray(spec_tiles)[matrix.reshape(-1),:,:].reshape(224,224)\n",
    "pyplot.figure(figsize=(6,6))\n",
    "pyplot.imshow(reshaped_spec, cmap='magma');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsl_to_rgb(h, s, l):\n",
    "    shape = h.shape\n",
    "    \n",
    "    # all inputs and outputs range 0..1\n",
    "    r = np.zeros(shape)\n",
    "    g = np.zeros(shape)\n",
    "    b = np.zeros(shape)\n",
    "    \n",
    "    # where the color is totally desaturated, use only luminance\n",
    "    grey = (s==0)\n",
    "    r[grey] = l[grey]\n",
    "    g[grey] = l[grey]\n",
    "    b[grey] = l[grey]\n",
    "    \n",
    "    # scale the saturation differently around medium luminance\n",
    "    q = np.zeros(shape)\n",
    "    low_luma = l < 0.5\n",
    "    q[low_luma] = l[low_luma] * (1 + s[low_luma])\n",
    "    hi_luma = l >= 0.5\n",
    "    q[hi_luma] = l[hi_luma] + s[hi_luma] - l[hi_luma] * s[hi_luma]\n",
    "    # the other hue factor is proportional\n",
    "    p = 2 * l - q;\n",
    "    \n",
    "    def channel(t):\n",
    "        # enforce limits\n",
    "        t[t < 0] += 1.0\n",
    "        t[t > 1] -= 1.0\n",
    "        x = np.zeros(shape)\n",
    "        tA = t < 1/6.\n",
    "        x[tA] = p[tA] + (q[tA] - p[tA]) * 6 * t[tA] \n",
    "        tB = (t >= 1/6.) & (t < 1/2.)\n",
    "        x[tB] = q[tB]\n",
    "        tC = (t >= 1/2.) & (t < 2/3.)\n",
    "        x[tC] = p[tC] + (q[tC] - p[tC]) * (2/3. - t[tC]) * 6\n",
    "        tD = t >= 2/3.\n",
    "        x[tD] = p[tD]\n",
    "        return x\n",
    "\n",
    "    chroma = (s != 0)\n",
    "    r[chroma] = channel(h + 1./3.)[chroma]\n",
    "    g[chroma] = channel(h)[chroma]\n",
    "    b[chroma] = channel(h - 1./3.)[chroma]\n",
    "\n",
    "    return r, g, b\n",
    "\n",
    "r2d, g2d, b2d = hsl_to_rgb(hue2d, sat2d, mag2d)\n",
    "image_rgb = np.stack((r2d, g2d, b2d), axis=2)\n",
    "plot.colorgram(image_rgb, x=step_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
